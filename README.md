# PySpark-Data-Analysis-Project

## Project Overview
### This project demonstrates how to use PySpark for handling and analyzing large datasets. PySpark is a powerful tool for distributed computing, and this project focuses on loading, exploring, and performing basic aggregations on data stored in CSV format. The goal is to showcase the use of PySpark's DataFrame API for scalable data analysis.

## Key Features

### Data Loading: Load CSV data into a Spark DataFrame.

Data Inspection: Inspect the structure and contents of the dataset using .show().

Aggregations: Perform group-by operations to calculate the sum and average of trade volumes by year.

SparkSession: Demonstrates how to set up and use a SparkSession for distributed data processing.

## Dataset

### he project uses a CSV file (data_p.csv) containing trade data, with columns like InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country. Additionally, data on stock trades is used to calculate the sum and average volume of trades by year.

## Technologies Used

### PySpark: Apache Spark's Python API for large-scale data processing.

### Jupyter Notebook: Interactive environment to run and document PySpark code.

## Conclusion

### This project demonstrates how PySpark can be used for efficient data analysis and aggregation on large datasets. It can serve as a foundation for more advanced data processing tasks, offering scalability and performance.


