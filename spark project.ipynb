{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157c7c92-a34a-41fd-b48c-a554e310b86c",
   "metadata": {},
   "source": [
    "# PySpark Project Notebook\n",
    "\n",
    "### This notebook demonstrates how to use PySpark to load, manipulate, and analyze data using Spark DataFrames. The data is loaded from CSV files, and various transformations and aggregations are applied using Spark's powerful distributed computing framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775eb14-67cc-4025-9545-14c2cfa080eb",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79459090-1319-49d6-a614-126f7d3a3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56383df8-2509-4e43-81fa-835746ddca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760bf0b-2412-4beb-9bcd-38fd85096ddf",
   "metadata": {},
   "source": [
    "## 2. Initializing SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8919af2-ae30-4231-8041-72ab5d7a1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aee9158-9699-444e-ac90-0faae9a94fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-I7HQU6N:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1fe7a143590>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f17e21-6f33-4fc6-a41a-8bc86cb5c8a1",
   "metadata": {},
   "source": [
    "## Loading Data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e46c2b24-445e-4228-a847-6efcc551f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.option(\"header\",\"true\").csv(\"data_p.csv\",inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab2d5b22-4b05-4ab1-8dec-618462109f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492921ef-c9e8-4537-bd06-c06a94879efc",
   "metadata": {},
   "source": [
    "## 1.Using\tSparkSession\tand\t‘data.csv’,\tprint\tall\tthe\tdistinct\tcountries\tin\tascending\torder\twith\t\n",
    "## 'an'\tin\ttheir\tname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d47fa08-78bd-4b21-a7e5-274e4def551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d3ba0ff-fcb7-4394-b0c2-4f974c68a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_countries = (\n",
    "    data.filter(col(\"Country\").contains(\"an\"))\n",
    "    .select(\"Country\")\n",
    "    .distinct()\n",
    "    .orderBy(\"Country\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "594d3864-dc38-4540-bd28-2a7288121116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           Country|\n",
      "+------------------+\n",
      "|            Canada|\n",
      "|   Channel Islands|\n",
      "|European Community|\n",
      "|           Finland|\n",
      "|            France|\n",
      "|           Germany|\n",
      "|           Iceland|\n",
      "|             Japan|\n",
      "|           Lebanon|\n",
      "|         Lithuania|\n",
      "|       Netherlands|\n",
      "|            Poland|\n",
      "|       Switzerland|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7bdbf-9b39-4945-87f7-f18b21ecdf60",
   "metadata": {},
   "source": [
    "## 2.Using\tSparkSession\tand\t‘data.csv’,\tshow\tthe\tInvoiceNo,\tStockCode,\tand\tDescription\tfor\tthe\t\n",
    "## highest\tunit\tprice.\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d623518-aa5f-4d62-a00d-3c0d42da3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,max\n",
    "max_price=data.agg(max(col(\"UnitPrice\"))).collect()[0][0]\n",
    "max_colunm=data.filter(col(\"UnitPrice\")==max_price )\n",
    "\n",
    "max_data=max_colunm.select(\"InvoiceNo\", \"StockCode\", \"Description\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceba3cac-8dd2-4d21-b813-8ca7047f98bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|InvoiceNo|StockCode|Description|\n",
      "+---------+---------+-----------+\n",
      "|  C556445|        M|     Manual|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060f81f-4412-41ca-baa9-3382dbfcfd97",
   "metadata": {},
   "source": [
    "## 3.Using\tSparkSession\tand\tthe\tile\tfakefriends-header.csv,\tShow\teach\tname's\ttotal\tnumber\tof\t\n",
    "## friends.\tOrder\tthe\tresults\tby\tname\tin\talphabetical\torder.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9af0098-b7e6-45e6-93be-32816c373a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake=spark.read.option(\"header\",\"True\").csv(\"fakefriends-header.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2031a1f1-390a-4580-9cb2-a7c7b5d93071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-------+\n",
      "|userID|    name|age|friends|\n",
      "+------+--------+---+-------+\n",
      "|     0|    Will| 33|    385|\n",
      "|     1|Jean-Luc| 26|      2|\n",
      "|     2|    Hugh| 55|    221|\n",
      "|     3|  Deanna| 40|    465|\n",
      "|     4|   Quark| 68|     21|\n",
      "|     5|  Weyoun| 59|    318|\n",
      "|     6|  Gowron| 37|    220|\n",
      "|     7|    Will| 54|    307|\n",
      "|     8|  Jadzia| 38|    380|\n",
      "|     9|    Hugh| 27|    181|\n",
      "|    10|     Odo| 53|    191|\n",
      "|    11|     Ben| 57|    372|\n",
      "|    12|   Keiko| 54|    253|\n",
      "|    13|Jean-Luc| 56|    444|\n",
      "|    14|    Hugh| 43|     49|\n",
      "|    15|     Rom| 36|     49|\n",
      "|    16|  Weyoun| 22|    323|\n",
      "|    17|     Odo| 35|     13|\n",
      "|    18|Jean-Luc| 45|    455|\n",
      "|    19|  Geordi| 60|    246|\n",
      "+------+--------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b778b1d7-f3f8-4a55-82da-f7ee95a4aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sum_friends=fake.groupBy(\"name\").sum(\"friends\")\n",
    "\n",
    "sum_friends=sum_friends.orderBy(\"name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9081dfde-15b2-40da-8d85-20dc71b1b810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   name|sum(friends)|\n",
      "+-------+------------+\n",
      "|    Ben|        4888|\n",
      "|Beverly|        6128|\n",
      "|  Brunt|        4805|\n",
      "|   Data|        7192|\n",
      "| Deanna|        3479|\n",
      "|  Dukat|        5317|\n",
      "|   Elim|        2541|\n",
      "|   Ezri|        4236|\n",
      "| Geordi|        4728|\n",
      "| Gowron|        2602|\n",
      "+-------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_friends.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549a86b-a25b-4152-a7e5-c09121908dc1",
   "metadata": {},
   "source": [
    "## 4.\tUsing\tSparkSession\tand\tthe\tile\tContainsNull.csv,\texplain\tthe\tsigni icance\tof\thow\tand\tthresh\targuments\tin\tdrop()\tfunction.\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d755594-e127-4c1a-8776-7586405e704c",
   "metadata": {},
   "source": [
    "### how Argument:\n",
    "#### The how argument specifies the condition for dropping rows:\n",
    "#### 1.\"any\": Drop a row if it contains any null values in any column\n",
    "#### 2.\"all\": Drop a row only if all its columns contain null values.\n",
    "\n",
    "### thresh Argument:\n",
    "#### The thresh argument sets a threshold for the minimum number of non-null values required to keep a row\n",
    "\n",
    "#### exapmle - For example, if you set thresh=2, a row must have at least 2 non-null values to be retained. If it has fewer, it will be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89be583-8719-4b41-8696-85c111c83cec",
   "metadata": {},
   "source": [
    "### 5. Using\tSparkSession\tand\tthe\tile\tContainsNull.csv,\till\tthe\tnull\tsales\tvalues\twith\tthe\tminimum\tsales\tvalue.\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "00d1f654-d0ac-4e27-90b5-04a62cabeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "null=spark.read.option(\"header\",\"True\").csv(\"ContainsNull.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "355d0b24-ded9-4128-a070-b5859a729cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "baa424de-4e9b-4695-98da-bf2e46118e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "min_values = null.agg(min(col('Sales'))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d14bc37-de1d-45d5-8de4-880b26527262",
   "metadata": {},
   "outputs": [],
   "source": [
    "null=data_filled = null.fillna( min_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39190f73-735c-42ef-8a23-a1fc128bf951",
   "metadata": {},
   "source": [
    "null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c91480-b92c-4326-8a67-62e2e988ff32",
   "metadata": {},
   "source": [
    "## 6.Using\tSparkSession\tand\tthe\tile\tappl_stock.csv,\tshow\tthe\tunique\ttrade\tyears\tin\tdescending\t\n",
    "## order\twith\tthe\toutput\tcolumn\tname\tas\tyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cfdd161f-9e58-4324-aaf1-75776621b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple=spark.read.option(\"header\",\"True\").csv(\"appl_stock.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3d1501ad-a23c-4958-87f7-ae2bd47faff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "af1eaee7-ba5c-4ec5-9ab9-c8503ea828ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,desc,count\n",
    "apple=apple.withColumn(\"year\",year(apple[\"Date\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b824b684-44b0-45b2-b495-69b7175c832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|year|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|2010|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|2010|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1274ee2b-e9fc-4967-9f34-ed9c3b0a0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value=apple.select(\"year\").distinct().orderBy(desc(\"year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4a09b602-7f2d-490a-9d5e-538850cd36e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2016|\n",
      "|2015|\n",
      "|2014|\n",
      "|2013|\n",
      "|2012|\n",
      "|2011|\n",
      "|2010|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_value.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c21c2-f91f-4819-bc43-6661e0f25212",
   "metadata": {},
   "source": [
    "## 7.Using\tSparkSession\tand\tthe\tile\tappl_stock.csv,\tshow\tthe\taverage\ttrade\tvolume\tfor\teach\tyear\t\n",
    "withthe\toutput\tcolumn\tnames\tand\tvalues\tas\tshown\tbelow.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0c6e2fe4-bce6-4260-9e85-6f7f4f057260",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tred=apple.groupBy(\"year\").sum(\"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "43590e74-2846-45c2-b445-ebc244925279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|sum(Volume)|\n",
      "+----+-----------+\n",
      "|2015|13063147500|\n",
      "|2013|25605392400|\n",
      "|2014|15914488100|\n",
      "|2012|32991051100|\n",
      "|2016| 9680671300|\n",
      "|2010|37756231800|\n",
      "|2011|31014834900|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_tred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b185ebd2-2612-4fca-9c8f-831cf7a50469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "avg_volume = apple.groupBy(\"year\").agg(F.format_number(F.avg(\"Volume\"), 2).alias(\"Final Avg Volume\")).orderBy(\"year\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a1e7e366-9001-4003-90f5-e8a9c7df123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+\n",
      "|year|Final Avg Volume|\n",
      "+----+----------------+\n",
      "|2010|  149,826,316.67|\n",
      "|2011|  123,074,741.67|\n",
      "|2012|  131,964,204.40|\n",
      "|2013|  101,608,700.00|\n",
      "|2014|   63,152,730.56|\n",
      "|2015|   51,837,886.90|\n",
      "|2016|   38,415,362.30|\n",
      "+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_volume.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc63f9-5ec2-430e-a3df-b64f14335924",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### This notebook demonstrated how to use PySpark for loading, exploring, and analyzing large datasets. Key tasks included importing data from a CSV, inspecting its structure, and performing aggregations like calculating the sum and average trade volumes by year. PySpark’s ability to handle big data efficiently makes it a powerful tool for scalable data analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
